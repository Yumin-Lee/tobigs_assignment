{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제: 네이버 영화 정보 및 평점 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대상: 예매순 상위 5개의 현재 상영 중인 영화\n",
    "- 수집할 항목: 영화 제목, 주연배우 3인, 네티즌 평점, 관람객 평점, 기자/평론가 평점, 관람객 별점 리뷰 20건 공감순으로(평점, 작성자닉네임, 리뷰본문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 예매순 상위 5개의 현재 상영 중인 영화 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 제목, 주연배우 3인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_title_url_actor():\n",
    "\n",
    "##### 1. 기본 설정\n",
    "    url = 'https://movie.naver.com/movie/running/current.nhn'\n",
    "    # 네이버 상영영화 url (예매순으로 보여 줌!)\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "##### 2.상위 5개 영화의 제목& 주연배우 3인 추출\n",
    "    \n",
    "    tit = soup.select('dt[class=\"tit\"] a', limit = 5) #limit로 5개만 추출합니다!\n",
    "    act = soup.select('span[class=\"link_txt\"]')\n",
    "    \n",
    "    titles = [] # 영화 제목 \n",
    "    actors = [] # 영화별 주연배우 3인\n",
    "\n",
    "    for i in range(5):\n",
    "        list_in = []\n",
    "        act_in = act[(3*i)+2]\n",
    "        \n",
    "        for j in act_in.select('a'):\n",
    "            list_in.append(j.get_text())\n",
    "            if len(list_in) == 3:\n",
    "                break # 주연배우가 3명 이하인 경우의 오류 방지!\n",
    "        titles.append(tit[i].get_text())\n",
    "        actors.append(list_in)\n",
    "        \n",
    "    return(titles, actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 해당 영화의 평점 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네티즌 평점, 관람객 평점, 기자/평론가 평점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grade():\n",
    "\n",
    "##### 1. 기본 설정\n",
    "    url = 'https://movie.naver.com/movie/running/current.nhn'\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "##### 2. 평점 가져올 url 할당\n",
    "    m = soup.select('dt[class=\"tit\"] a', limit=5) # 상위 5개의 영화 받음\n",
    "    \n",
    "    m_url = []\n",
    "    for i in range(len(m)):\n",
    "        m_url.append(\"https://movie.naver.com\" + m[i].attrs['href'])\n",
    "    m_url # 5개 영화 각각의 페이지 주소를 불러 옴\n",
    "    \n",
    "##### 3. 평점 받아오기\n",
    "    score1 = [] # 네티즌\n",
    "    score2 = [] # 관람객\n",
    "    score3 = [] # 기자/평론가\n",
    "\n",
    "    for i in m_url:\n",
    "        url = i\n",
    "        res = requests.get(url)\n",
    "        html = res.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        review = soup.select('div[class=\"star_score\"] em', limit=12)\n",
    "        hb = []\n",
    "    \n",
    "        for j in range(len(review)):\n",
    "            hb.append(review[j].get_text())\n",
    "        score1.append(hb[0]+hb[1]+hb[2]+hb[3])\n",
    "        score2.append(hb[4]+hb[5]+hb[6]+hb[7])\n",
    "        score3.append(hb[8]+hb[9]+hb[10]+hb[11])\n",
    "    \n",
    "    return(score1, score2, score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 관람객 평점 공감순 20건 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평점, 평점 작성자 닉네임, 리뷰 본문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_reviews 함수는\n",
    " 1. 네이버 상영영화 페이지에서 정규표현식으로 각 영화별 6자리 코드를 추출한 후\n",
    " 2. 각 영화별 평점 페이지 url 할당\n",
    " 3. 영화별로 20개의 평점, 평점 작성자 닉네임, 리뷰 본문 받아오기\n",
    "   \n",
    "   순서로 작성했습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews():\n",
    "    # 기본 설정\n",
    "    url = 'https://movie.naver.com/movie/running/current.nhn'\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    \n",
    "##### 1. 영화별 코드 받아오기 by 정규표현식 \n",
    "    m_code = soup.select('dt[class=\"tit\"] a', limit = 5)\n",
    "     # m_code에는 아래와 같이 들어와 있으므로 \n",
    "     # <a href=\"/movie/bi/mi/basic.nhn?code=181925\">클로젯</a>\n",
    "     # 각 영화별로 code를 추출합시당\n",
    "        \n",
    "    codes = []\n",
    "    for i in range(len(m_code)):\n",
    "        p = re.compile('[0-9]{6}')\n",
    "        codes.append(p.findall(str(m_code[i])))\n",
    "\n",
    "        \n",
    "##### 2. 각 영화마다 평점 페이지 2개씩 추출 (한페이지에 리뷰가 10개니까!)\n",
    "    review_url = []\n",
    "    for i in range(len(codes)):\n",
    "        for j in range(2):\n",
    "            review_url.append(\"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=\" + str(codes[i][0]) + \"&type=after&onlyActualPointYn=N&onlySpoilerPointYn=N&order=sympathyScore&page=\" + str(j+1))\n",
    "            # 총 10(5*2)개의 페이지 주소 추출!\n",
    "\n",
    "\n",
    "##### 3.1. 영화별 평점 추출\n",
    "    indiv_score = [] # 영화당 20개씩, 총 100개의 평점 받을 list 생성\n",
    "    \n",
    "    for i in range(len(review_url)): \n",
    "        url = review_url[i]\n",
    "        res = requests.get(url)\n",
    "        html = res.text\n",
    "        soup = BeautifulSoup(html)\n",
    "    \n",
    "        m_score = soup.select('div[class=\"star_score\"]', limit = 10)\n",
    "        for j in range(len(m_score)):\n",
    "            indiv_score.append(m_score[j].get_text().strip())\n",
    "    \n",
    "    # 20개씩 묶어줍시다\n",
    "    final_score = []\n",
    "    for i in [0, 20, 40, 60, 80]:\n",
    "        final_score.append(indiv_score[i:i+20])\n",
    "        # 각 영화별 평점 20개 저장!\n",
    "\n",
    "\n",
    "##### 3.2. 영화별 리뷰 & 닉네임 추출\n",
    "####### 1) 리뷰& 닉네임 받아오기\n",
    "    review_nick = []\n",
    "    \n",
    "    for i in range(len(review_url)): \n",
    "        url = review_url[i]\n",
    "        res = requests.get(url)\n",
    "        html = res.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        re_ni = soup.select('div[class=\"score_reple\"] span')\n",
    "        for j in range(len(re_ni)):\n",
    "            review_nick.append(re_ni[j].get_text().strip())\n",
    "\n",
    "####### 2) 필요없는 인자 제거 \n",
    "    # 함수 정의\n",
    "    def remove_values(mylist, val):\n",
    "        return [value for value in mylist if value != val]\n",
    "    \n",
    "    # 제거 \n",
    "    review_nick = remove_values(review_nick, \"관람객\")# 리스트에서 \"관람객\"인 경우 제거!\n",
    "    review_nick = remove_values(review_nick, \"스포일러가 포함된 감상평입니다. 감상평 보기\") # 필요없음!\n",
    "    \n",
    "####### 3) 길이가 200이 아닌 경우 중복 제거 \n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    review_nick = list(OrderedDict.fromkeys(review_nick))\n",
    "    \n",
    "    review = [] # 리뷰\n",
    "    m_id = [] #개인평점\n",
    "    for i in range(len(review_nick)):\n",
    "        if i%2 == 0 :\n",
    "            review.append(review_nick[i])\n",
    "        else :\n",
    "            m_id.append(review_nick[i])\n",
    "\n",
    "    final_id = []\n",
    "    final_review = []\n",
    "    for i in [0, 20, 40, 60, 80]:\n",
    "        final_id.append(m_id[i:i+20])\n",
    "        final_review.append(review[i:i+20])\n",
    "        # 각 영화별로 20개씩 저장!\n",
    "    \n",
    "    return(final_score, final_id, final_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    movie = DataFrame()\n",
    "    movie['영화제목'] = data1[0]\n",
    "    movie['주연배우 3인'] = data1[1]\n",
    "    movie['평점'] = data2[0]    \n",
    "    movie['개인 평점'] = data3[0]\n",
    "    movie['닉네임/아이디'] = data3[1]\n",
    "    movie['리뷰'] = data3[2]\n",
    "    movie.to_pickle('영화 크롤링1')\n",
    "    movie.to_excel('영화 크롤링2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 크롤링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = movie_title_url_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = get_grade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data3 = get_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
